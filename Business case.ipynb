{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bd29dab-6232-4941-af55-e41767233d09",
   "metadata": {},
   "source": [
    "#### Business Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c309f0-3486-4a2f-8e1e-a4103b9596e3",
   "metadata": {},
   "source": [
    "Given data from an audio book app, we want to create a ML algorithm based on our data to predict if a customer will buy again from the audio group company. \n",
    "\n",
    "The main idea is for the company not to spend its advertising budget targeting individuals unlikely to come back.\n",
    "\n",
    "The input data represents 2 years worth of engagement and 6 months of data to check conversion,\n",
    "\n",
    "Targets: \n",
    "- 1 if a customer bought again in the last 6 months of data.\n",
    "- 0 if a customer did not buy again.\n",
    "\n",
    "##### Task\n",
    "\n",
    "Create a ML algorithm that can predict if a customer will buy again.\n",
    "\n",
    "This is a classification problem with 2 classes - wont buy and will buy represented buy 0 and 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bff63cf-41bc-4d89-880f-d60a923d3924",
   "metadata": {},
   "source": [
    "#### The Business Case Action Plan\n",
    "\n",
    "1. Preprocess the data.\n",
    "    \n",
    "    3 steps to doing this are:\n",
    "    \n",
    "    i. Balance the dataset\n",
    "    \n",
    "    ii. Divide the dataset into 3 parts - training, validation and test.\n",
    "    \n",
    "    iii. Save the dataset in a tensor friendly format\n",
    "    \n",
    "2. Create the Machine Learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792b072d-fd93-4622-851e-af23e3b6547d",
   "metadata": {},
   "source": [
    "#### Balancing the dataset\n",
    "\n",
    "Importance of balancing your dataset.\n",
    "\n",
    "90% accuracy for most problem is an impressive accomplishment.\n",
    "\n",
    "The initial probability of picking one of 2 categories of data are referred to as a prior. The priors are balance when the 2 categories are 50% each.\n",
    "\n",
    "Examples of unbalanced priors are\n",
    "- 90% and 10%\n",
    "- 70% and 30%\n",
    "- 60% and 40%\n",
    "\n",
    "For 3 classes - 33% each\n",
    "For 4 classes - 25% each\n",
    "\n",
    "In ML, only a result above 90% result is a more favouble one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba793352-ee68-419f-8440-4db033ad5d3f",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bb56a4-1a1a-4ce7-a225-7f0810f8953e",
   "metadata": {},
   "source": [
    "#### Practical example. Audiobooks\n",
    "\n",
    "Preprocess the data, Balance the dataset, create 3 datasets: training, validation and test. Save the newly created sets in a tensor friendly format (e.g. *.npz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e95ebde-6b15-40eb-91b2-26a2deeef63c",
   "metadata": {},
   "source": [
    "##### Extract the data from the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5853ed6c-4e6c-4c22-9845-20d1410050c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.1-cp310-cp310-win_amd64.whl (8.3 MB)\n",
      "     ---------------------------------------- 8.3/8.3 MB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from scikit-learn) (1.23.5)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\user\\anaconda3\\envs\\py3-tf2.0\\lib\\site-packages (from scikit-learn) (1.10.0)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "     -------------------------------------- 298.0/298.0 kB 1.5 MB/s eta 0:00:00\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.2.1 threadpoolctl-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af2fc34c-d5a4-4172-b404-7f195cff2597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#using the sklearn capabilities for standardizing the inputs\n",
    "#Load the csv file\n",
    "raw_csv_data = np.loadtxt('Audiobooks_data.csv', delimiter = ',')\n",
    "\n",
    "unscaled_inputs_all = raw_csv_data[:, 1:-1]\n",
    "targets_all = raw_csv_data[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd6bd5-800a-43e0-82b9-a9021c7feeb2",
   "metadata": {},
   "source": [
    "##### Balance the dataset\n",
    "1. We will count the number of targets that are 1s - if we sum all the targets, we will get the number of targets that are 1s.\n",
    "2. We will keep as many 0s as 1s (we will delete the others)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79934d13-35f0-4dfe-998b-d2cfad0fd318",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_targets = int(np.sum(targets_all))\n",
    "zero_targets_counter = 0\n",
    "indices_to_remove = []\n",
    "\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] ==0: #we want to increase the 0s counter by 1, if the target is 0\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "            \n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis = 0) #a method that deletes an object along an axis\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis = 0)\n",
    "\n",
    "#We have a balanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8879a12-c76d-4280-adaf-c4cce4191286",
   "metadata": {},
   "source": [
    "#### Standardize the inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fc8392e-26d7-49d0-8832-3c1c5ef30f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors) #a method that standardizes an array along an axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda2511b-386f-4b4c-bbe8-74d331cb6d6e",
   "metadata": {},
   "source": [
    "#### Shuffle the data\n",
    "\n",
    "A little trick is to shuffle the inputs and the targets. We keep same info but in a random order. We must shuffle the data since we will be batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98a19e22-3ce5-4484-98c6-e225100800e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.arange(scaled_inputs.shape[0])  #a method that returns a evenly spaced values within a given interval\n",
    "np.random.shuffle(shuffled_indices) # a method that shuffles the numbersin a given sequence\n",
    "\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices] \n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9ca5cf-1a88-4b74-995e-62a5c109b9dd",
   "metadata": {},
   "source": [
    "#### Split the dataset into train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4954557a-8f4c-4941-9877-20414a781bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1801.0 3579 0.5032131880413523\n",
      "226.0 447 0.5055928411633109\n",
      "210.0 448 0.46875\n"
     ]
    }
   ],
   "source": [
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "#determine the size of each dataset. We'll be using 80-10-10.\n",
    "train_samples_count = int(0.8*samples_count)\n",
    "validation_samples_count = int(0.1*samples_count)\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "#Extractracting them from the big dataset\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "validation_inputs = shuffled_inputs[train_samples_count : train_samples_count + validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count : train_samples_count + validation_samples_count]\n",
    "\n",
    "test_inputs = shuffled_inputs[train_samples_count + validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count + validation_samples_count:]\n",
    "\n",
    "#it is useful to check that the dataset is balanced\n",
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6d72c6-b006-497c-a3e4-2752d1d3552c",
   "metadata": {},
   "source": [
    "Explaining the result:\n",
    "\n",
    "The training set is considerably larger than the validation and the test - just the way we want it. \n",
    "\n",
    "The priors look ok as all three sets are balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31c35ce-71fe-4f6e-b85b-446f6332474a",
   "metadata": {},
   "source": [
    "#### Save the three datasets in *.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca0d84e4-e0cb-4500-8eb5-28b862ffdc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobooks_data_train', inputs = train_inputs, targets = train_targets)\n",
    "np.savez('Audiobooks_data_validation', inputs = validation_inputs, targets = validation_targets)\n",
    "np.savez('Audiobooks_data_test', inputs = test_inputs, targets = test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d37137-bb31-4c31-8d44-caf353ddd9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53bb1c7-01ff-4377-bb7d-496731ff0aab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7500b0a-caea-4f38-a941-64127d6dfde8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
